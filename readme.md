# Transformers Almost From Scratch

This project implements the foundational components of the "Attention Is All You Need" paper to build a functional Transformer model from scratch. The code is written in PyTorch and focuses on understanding and implementing the core concepts of self-attention, multi-head attention, and Transformer blocks. It is missing core training setup features like model saving, loading, metric tracking, plotting, configs, etc. This is because it's main goal is to understand better the core concepts behind the transformer architecture. Most of the code was based on andrej karpathy's build a gpt video. 

With more training time and deeper architecture better outputs should be achieavable. 

## File Structure

- **`self_attention.py`**: Core implementation of the Transformer components:
  - `SelfAttentionHead`: Implements a single attention head with causal masking.
  - `MultiHeadAttention`: Combines multiple attention heads into a single module.
  - `FeedForward`: A simple feedforward network used in Transformer blocks.
  - `Block`: A Transformer block combining multi-head attention and feedforward layers.
  - Includes example usage and testing in the `__main__` block.

- **`models.py`**: Defines the `SmallLanguageModel`, a Transformer-based language model with:
  - Token and positional embeddings.
  - Stacked Transformer blocks for sequence modeling.
  - A language modeling head for predicting the next token.
  - A `generate` method for autoregressive text generation.

- **`tokenizers.py`**: Implements tokenization utilities:
  - `BaseTokenizer`: Abstract base class for tokenizers.
  - `SimpleTokenizer`: A character-level tokenizer that maps characters to integers and vice versa.

- **`dataset/text_dataset.py`**: Defines the `TextDataset` class for:
  - Loading and preprocessing text data.
  - Splitting data into training and validation sets.
  - Generating batches of tokenized sequences for training.

- **`train.py`**: Script to train the `SmallLanguageModel` on a text dataset. It includes:
  - Dataset loading and tokenization using `TextDataset` and `SimpleTokenizer`.
  - Model initialization and training loop with loss computation and optimization.
  - Text generation using the trained model.

- **`dataset/input.txt`**: The input text file used for training. This file contains raw text data that is tokenized and processed into sequences for the model.

- **`example_output.txt`**: Example output generated by the trained model, showcasing its ability to generate text based on the training data.

- **`.gitignore`**: Specifies files and directories to be ignored by Git, such as `__pycache__` and `.venv`.

## Key Features

- Implements self-attention with causal masking to prevent attending to future tokens.
- Supports multi-head attention with configurable parameters for embedding size, number of heads, and block size.
- Includes a Transformer block combining attention and feedforward layers.
- Provides a training script to train the Transformer model on custom datasets.
- Includes a simple character-level tokenizer for text processing.
- Designed for educational purposes to understand the basics of Transformers.

## Requirements

- Python 3.8+
- PyTorch 1.10+
- tqdm

## Training Setup

1. **Prepare the Dataset**:
   - Place your text data in `dataset/input.txt`. Each line should represent a sequence of text.
   - The `SimpleTokenizer` in `train.py` will process the text into tokens and create a vocabulary.

2. **Run the Training Script**:
   - Configure hyperparameters in `train.py` (e.g., `BATCH_SIZE`, `BLOCK_SIZE`, `LEARNING_RATE`, etc.).
   - Start training:
     ```bash
     python train.py
     ```

3. **Monitor Training**:
   - The script logs training and validation loss at regular intervals.
   - Checkpoints are saved periodically for resuming or evaluating the model.

4. **Generate Text**:
   - After training, the script can generate new text sequences using the trained model:
   - The generated text will be printed to the console.

## Usage

To test the individual components, run the self_attention.py file:
```bash
python self_attention.py
```

To train the Transformer model, use the train.py script:
```bash
python train.py
```

## Output

- The self_attention.py script prints the shapes of the outputs for `SelfAttentionHead`, `MultiHeadAttention`, and `Block` to verify correctness.
- The train.py script outputs training progress, including loss and evaluation metrics, and generates text samples.
- Example generated text is saved in example_output.txt.

## References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
